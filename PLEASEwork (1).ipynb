{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "997a22de69fa45bd8b4b20c10e7e05c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7d5c4bc678b4a0192a0a308341fd560",
              "IPY_MODEL_34cc0be91a2a48e09d8b2f5c0e0a81aa",
              "IPY_MODEL_09478b32cb1b41e5bec009e67603fbc5"
            ],
            "layout": "IPY_MODEL_fb7cb3bacb7f4ab59af5277a9a71af54"
          }
        },
        "a7d5c4bc678b4a0192a0a308341fd560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_205cfef92fbd4198b7ac0e510f01377c",
            "placeholder": "​",
            "style": "IPY_MODEL_d4ac98f4b22f4e56a1acbf0417dee1c7",
            "value": "100%"
          }
        },
        "34cc0be91a2a48e09d8b2f5c0e0a81aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f21312d5f6f44ece90dcc15585a9ef68",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4a7f3b6a11446298c577f0374c57bd7",
            "value": 20
          }
        },
        "09478b32cb1b41e5bec009e67603fbc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae2ff5d360854a2aa550934daf16aca4",
            "placeholder": "​",
            "style": "IPY_MODEL_af83f1847fa24a84b25a7eb2349412fb",
            "value": " 20/20 [00:02&lt;00:00,  7.59it/s]"
          }
        },
        "fb7cb3bacb7f4ab59af5277a9a71af54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "205cfef92fbd4198b7ac0e510f01377c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ac98f4b22f4e56a1acbf0417dee1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f21312d5f6f44ece90dcc15585a9ef68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4a7f3b6a11446298c577f0374c57bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae2ff5d360854a2aa550934daf16aca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af83f1847fa24a84b25a7eb2349412fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0HyyNqMQ3ImO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b26949-67ad-4257-89b8-2128e628349c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.3)\n",
            "fatal: destination path 'stylegan2-ada-pytorch' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'stylegan2-ada-pytorch # Navigate to the cloned repository'\n",
            "/content\n",
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# Install required packages\n",
        "!pip install ninja\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "%cd stylegan2-ada-pytorch # Navigate to the cloned repository\n",
        "!python setup.py build --force  # Build the custom CUDA kernels for StyleGAN2-ADA\n",
        "!python setup.py install # Install the repository and its dependencies, including dnnlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "# Add the path to the stylegan2-ada-pytorch repo\n",
        "sys.path.append('/content/stylegan2-ada-pytorch') # Append the path to the repository\n",
        "import dnnlib\n",
        "import legacy\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gzip\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "metadata": {
        "id": "G1lfiFF_3xeL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained StyleGAN2-ADA model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_path = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
        "\n",
        "with dnnlib.util.open_url(model_path) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device)  # Load the generator\n",
        "\n",
        "\n",
        "# !pip install ninja\n",
        "# !git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "# %cd stylegan2-ada-pytorch\n",
        "\n",
        "!pip install git+https://github.com/FacePerceiver/facer.git@main\n",
        "\n",
        "# Install and set up pixel2style2pixel for encoding real images to StyleGAN latent space\n",
        "!git clone https://github.com/eladrich/pixel2style2pixel.git\n",
        "%cd pixel2style2pixel\n",
        "!python setup.py develop\n",
        "%cd ..\n",
        "\n",
        "\n",
        "# Download the pretrained pSp model for face encoding\n",
        "!mkdir -p pretrained_models\n",
        "!gdown --id 1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0 -O pretrained_models/psp_ffhq_encode.pt\n",
        "\n",
        "# Download the pretrained StyleGAN model if needed\n",
        "!gdown --id 1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT -O pretrained_models/stylegan2-ffhq-config-f.pt\n",
        "\n",
        "# Install custom CUDA kernels for StyleGAN2\n",
        "!python setup.py develop\n",
        "\n",
        "\n",
        "\n",
        "# Add the pSp repository to the path\n",
        "import sys\n",
        "sys.path.append(\"pixel2style2pixel\")\n",
        "\n",
        "# Import pSp modules\n",
        "from models.psp import pSp\n",
        "from utils.common import tensor2im\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AISWGdnU34G1",
        "outputId": "0d126158-be9c-4d1d-8f7c-d12420f0c8b3",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/FacePerceiver/facer.git@main\n",
            "  Cloning https://github.com/FacePerceiver/facer.git (to revision main) to /tmp/pip-req-build-6q_6j_6r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/FacePerceiver/facer.git /tmp/pip-req-build-6q_6j_6r\n",
            "  Resolved https://github.com/FacePerceiver/facer.git to commit ddd35c76ff840174b8a5403ad1c1255e37b8782b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (0.20.1+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (1.0.15)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (11.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (1.26.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (7.7.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (3.10.0)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (0.34.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (2.32.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from pyfacer==0.0.5) (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->pyfacer==0.0.5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.1->pyfacer==0.0.5) (1.3.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->pyfacer==0.0.5) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->pyfacer==0.0.5) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->pyfacer==0.0.5) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->pyfacer==0.0.5) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->pyfacer==0.0.5) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->pyfacer==0.0.5) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyfacer==0.0.5) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyfacer==0.0.5) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyfacer==0.0.5) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyfacer==0.0.5) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyfacer==0.0.5) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyfacer==0.0.5) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyfacer==0.0.5) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pyfacer==0.0.5) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pyfacer==0.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pyfacer==0.0.5) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pyfacer==0.0.5) (2025.1.31)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pyfacer==0.0.5) (1.13.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pyfacer==0.0.5) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pyfacer==0.0.5) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->pyfacer==0.0.5) (0.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm->pyfacer==0.0.5) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->pyfacer==0.0.5) (0.28.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->pyfacer==0.0.5) (0.5.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pyfacer==0.0.5) (1.17.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (6.5.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->pyfacer==0.0.5) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->pyfacer==0.0.5) (3.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (5.7.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->pyfacer==0.0.5) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->pyfacer==0.0.5) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (0.23.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->pyfacer==0.0.5) (1.3.1)\n",
            "fatal: destination path 'pixel2style2pixel' already exists and is not an empty directory.\n",
            "/content/pixel2style2pixel\n",
            "python3: can't open file '/content/pixel2style2pixel/setup.py': [Errno 2] No such file or directory\n",
            "/content\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0\n",
            "From (redirected): https://drive.google.com/uc?id=1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0&confirm=t&uuid=f5dc5ef3-98a1-4f17-a69c-ae8b02f698dd\n",
            "To: /content/pretrained_models/psp_ffhq_encode.pt\n",
            "100% 1.20G/1.20G [00:09<00:00, 122MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT\n",
            "From (redirected): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT&confirm=t&uuid=ce167441-7dc7-480b-8076-21eecfdcce18\n",
            "To: /content/pretrained_models/stylegan2-ffhq-config-f.pt\n",
            "100% 381M/381M [00:03<00:00, 119MB/s] \n",
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "1C2QbeA_5B2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"# Preprocessing\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def import_image(image_path):\n",
        "  \"\"\"Imports an image from the specified path using Pillow library.\n",
        "\n",
        "  Args:\n",
        "    image_path: The path to the image file.\n",
        "\n",
        "  Returns:\n",
        "    A PIL Image object if successful, otherwise None.\n",
        "  \"\"\"\n",
        "  base_path = \"/content/drive/MyDrive/Child Generator/\"\n",
        "  image_path = os.path.join(base_path, image_path)\n",
        "  try:\n",
        "    img = Image.open(image_path)\n",
        "    return img\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Image file not found at {image_path}\")\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading image: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_family(family_id, no_of_images=1):\n",
        "  \"\"\"\n",
        "  Retrieves family information for a given family ID.\n",
        "\n",
        "  Args:\n",
        "    family_id: The ID of the family.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing family information, or None if the family ID is not found.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/Child Generator/CSVs/checkpoint3.csv\")\n",
        "    family_data = df[df['family_id'] == family_id].iloc[0]\n",
        "\n",
        "    father_images = random.sample(family_data['father_images'].split(';'), min(no_of_images, len(family_data['father_images'].split(';'))))\n",
        "    mother_images = random.sample(family_data['mother_images'].split(';'), min(no_of_images, len(family_data['mother_images'].split(';'))))\n",
        "    child_images = random.sample(family_data['child_images'].split(';'), min(no_of_images, len(family_data['child_images'].split(';'))))\n",
        "\n",
        "    family_info = {\n",
        "        'father_images': father_images,\n",
        "        'mother_images': mother_images,\n",
        "        'child_images': child_images,\n",
        "        'Father_name': family_data['father_name'],\n",
        "        'Mother_name': family_data['mother_name'],\n",
        "        'Child_name': family_data['child_name']\n",
        "    }\n",
        "    return family_info\n",
        "  except IndexError:\n",
        "    print(f\"Family with ID {family_id} not found.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Sample usage to get a path for say 3 trios\n",
        "\"\"\"\n",
        "\n",
        "fathers = []\n",
        "mothers = []\n",
        "children = []\n",
        "\n",
        "for i in range(10):\n",
        "  family = get_family(i, no_of_images=1)\n",
        "  fathers.append(family.get('father_images'))\n",
        "  mothers.append(family.get('mother_images'))\n",
        "  children.append(family.get('child_images'))\n",
        "\n",
        "print(fathers)\n",
        "print(mothers)\n",
        "print(children)\n",
        "\n",
        "\"\"\"\n",
        "Sample usage to get an image from a path without calling import image\n",
        "\"\"\"\n",
        "\n",
        "def append_to_base_path(path):\n",
        "  base_path = \"/content/drive/MyDrive/Child Generator/\"\n",
        "  return os.path.join(base_path, path)\n"
      ],
      "metadata": {
        "id": "l8bSblRthu6y",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8e7f02-b28a-49d3-b3ec-b460aa260753"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[['Families/Obama/Barack_Obama/Barack_Obama_20403.jpg'], ['Families/Obama/Barack_Obama/Barack_Obama_20091.jpg'], ['Families/Obama/Barack_Obama_Sr/Barack_Obama_Sr_00002.jpg'], ['Families/Obama/Lolo_Soetoro/Lolo_Soetoro_00001.jpg'], ['Families/Obama/Stanley_Dunham/Stanley_Dunham_00002.jpg'], ['Families/Bush/Jeb_Bush/Jeb_Bush_0030.jpg'], ['Families/Clinton/Bill_Clinton/Bill_Clinton_0029.jpg'], ['Families/Clinton/William_Jefferson_Blythe/William_Jefferson_Blythe_0001.jpg'], ['Families/Reagan/Ronald_Reagan/Ronald_Reagan_0023.jpg'], ['Families/Reagan/Ronald_Reagan/Ronald_Reagan_0012.jpg']]\n",
            "[['Families/Obama/Michelle_Obama/Michelle_Obama_00007.jpg'], ['Families/Obama/Michelle_Obama/Michelle_Obama_00015.jpg'], ['Families/Obama/Ann_Dunham/Ann_Dunham_00013.jpg'], ['Families/Obama/Ann_Dunham/Ann_Dunham_00005.jpg'], ['Families/Obama/Madelyn_Payne/Madelyn_Payne_00002.jpg'], ['Families/Bush/Columba_Bush/Columba_Bush_0001.jpg'], ['Families/Clinton/Hillary_Clinton/Hillary_Clinton_0005.jpg'], ['Families/Clinton/Virginia_Clinton_Kelley/Virginia_Clinton_Kelley_0004.jpg'], ['Families/Reagan/Jane_Wyman/Jane_Wyman_0020.jpg'], ['Families/Reagan/Nancy_Reagan/Nancy_Reagan_0014.jpg']]\n",
            "[['Families/Obama/Malia_Ann_Obama/Malia_Ann_Obama_00013.jpg'], ['Families/Obama/Natasha_Obama/Natasha_Obama_00019.jpg'], ['Families/Obama/Barack_Obama/Barack_Obama_20304.jpg'], ['Families/Obama/Maya_Soetoro/Maya_Soetoro_00021.jpg'], ['Families/Obama/Ann_Dunham/Ann_Dunham_00006.jpg'], ['Families/Bush/George_Prescott_Bush/George_Prescott_Bush_0008.jpg'], ['Families/Clinton/Chelsea_Clinton/Chelsea_Clinton_0001.jpg'], ['Families/Clinton/Bill_Clinton/Bill_Clinton_0001.jpg'], ['Families/Reagan/Maureen_Reagan/Maureen_Reagan_0008.jpg'], ['Families/Reagan/Patti_Davis/Patti_Davis_0001.jpg']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_psp_encoder(device):\n",
        "    \"\"\"Load the pretrained pSp encoder model\"\"\"\n",
        "    model_path = 'pretrained_models/psp_ffhq_encode.pt'\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Model file not found. Downloading...\")\n",
        "        os.makedirs('pretrained_models', exist_ok=True)\n",
        "        !gdown --id 1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0 -O {model_path}\n",
        "\n",
        "    try:\n",
        "        ckpt = torch.load(model_path, map_location=device)\n",
        "        opts = ckpt['opts']\n",
        "\n",
        "        # Convert opts to namespace and add required attributes\n",
        "        from argparse import Namespace\n",
        "        opts = Namespace(**opts)\n",
        "\n",
        "        # Add missing attributes\n",
        "        opts.output_size = 1024\n",
        "        opts.checkpoint_path = model_path\n",
        "        opts.device = device\n",
        "        opts.stylegan_weights = 'pretrained_models/stylegan2-ffhq-config-f.pt'\n",
        "        opts.stylegan_size = 1024\n",
        "        opts.start_from_latent_avg = True\n",
        "        opts.learn_in_w = True\n",
        "\n",
        "        # Ensure StyleGAN weights exist\n",
        "        if not os.path.exists(opts.stylegan_weights):\n",
        "            print(\"StyleGAN weights not found. Downloading...\")\n",
        "            !gdown --id 1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT -O {opts.stylegan_weights}\n",
        "\n",
        "        # Create the pSp model\n",
        "        from models.psp import pSp\n",
        "        psp_encoder = pSp(opts).to(device).eval()\n",
        "        print(\"pSp encoder loaded successfully\")\n",
        "        return psp_encoder\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pSp encoder: {str(e)}\")\n",
        "        print(\"Please ensure you have the pixel2style2pixel repository cloned and set up correctly\")\n",
        "        return None\n",
        "\n",
        "def get_transforms():\n",
        "    \"\"\"Define image transformations for pSp encoder\"\"\"\n",
        "    transforms_dict = {\n",
        "        'transform': transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "    }\n",
        "    return transforms_dict"
      ],
      "metadata": {
        "id": "WwsL2bKV4pnu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "def get_latent_vector(img_path, encoder, device):\n",
        "    \"\"\"Extract latent vector from image using pSp encoder\n",
        "\n",
        "    Args:\n",
        "        img_path: Path to image or PIL Image\n",
        "        encoder: pSp encoder model (not the generator)\n",
        "        device: Torch device\n",
        "\n",
        "    Returns:\n",
        "        W+ latent vector (1 x 18 x 512)  # StyleGAN2 expects this shape\n",
        "    \"\"\"\n",
        "    if isinstance(img_path, str):\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "    else:\n",
        "        img = img_path.convert('RGB')\n",
        "\n",
        "    # Transform image for pSp encoder\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Extract latent vector using pSp encoder\n",
        "    with torch.no_grad():\n",
        "        # The encoder returns (images, latents)\n",
        "        _, latents = encoder(img_tensor, randomize_noise=False, return_latents=True)\n",
        "        # latents should now be shape [1, 18, 512]\n",
        "\n",
        "    print(f\"W+ shape: {latents.shape}\")\n",
        "    return latents\n"
      ],
      "metadata": {
        "id": "4XOhipwT77lV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average in W+ space with equal weights\n",
        "def slerp(t, v0, v1):\n",
        "    \"\"\"Spherical interpolation between v0 and v1.\"\"\"\n",
        "    v0_norm = v0 / torch.norm(v0, dim=-1, keepdim=True)\n",
        "    v1_norm = v1 / torch.norm(v1, dim=-1, keepdim=True)\n",
        "    omega = torch.acos((v0_norm * v1_norm).sum(dim=-1, keepdim=True))\n",
        "    sin_omega = torch.sin(omega)\n",
        "\n",
        "    return (torch.sin((1 - t) * omega) / sin_omega) * v0 + (torch.sin(t * omega) / sin_omega) * v1\n",
        "\n",
        "def combine_parents(father_path, mother_path, encoder, generator, device):\n",
        "    \"\"\"Combine parent images in W+ latent space\n",
        "\n",
        "    Args:\n",
        "        father_path: Path to father image\n",
        "        mother_path: Path to mother image\n",
        "        encoder: pSp encoder for extracting latent vectors\n",
        "        generator: StyleGAN2 generator for generating images from latent vectors\n",
        "        device: Torch device\n",
        "\n",
        "    Returns:\n",
        "        Combined parent image, Combined W+ latent vector\n",
        "    \"\"\"\n",
        "    # Get parent latent vectors using the encoder\n",
        "    father_latent = get_latent_vector(father_path, encoder, device)\n",
        "    mother_latent = get_latent_vector(mother_path, encoder, device)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print(f\"Father latent shape: {father_latent.shape}\")\n",
        "    print(f\"Mother latent shape: {mother_latent.shape}\")\n",
        "\n",
        "    # Interpolate between parents\n",
        "    t = 0.5  # 0 = father, 1 = mother\n",
        "    combined_latent = slerp(t, father_latent, mother_latent)\n",
        "\n",
        "    # Generate combined image using the generator\n",
        "    with torch.no_grad():\n",
        "        # StyleGAN2 expects latents of shape [1, 18, 512]\n",
        "        combined_image = generator.synthesis(combined_latent, noise_mode='const')\n",
        "\n",
        "    # Convert to PIL image\n",
        "    combined_image = combined_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "    combined_image = ((combined_image + 1) * 127.5).astype(np.uint8)\n",
        "    combined_image = Image.fromarray(combined_image)\n",
        "\n",
        "    return combined_image, combined_latent #TODO make sure we are using this combined latent later instead of reextracting it again"
      ],
      "metadata": {
        "id": "_EmqAGUG7__t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pSp encoder\n",
        "psp_encoder = load_psp_encoder(device)\n",
        "\n",
        "# Main parent combination loop\n",
        "parent_images = []\n",
        "parent_latents = []\n",
        "parent_image_paths = []  # Add this list to store paths\n",
        "output_dir = \"generated_parents\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for i, (father, mother) in enumerate(zip(fathers, mothers)):\n",
        "    father_path = append_to_base_path(father[0])\n",
        "    mother_path = append_to_base_path(mother[0])\n",
        "\n",
        "    # Combine parents using both encoder and generator\n",
        "    combined_image, combined_latent = combine_parents(\n",
        "        father_path, mother_path, psp_encoder, G, device\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    father_name = os.path.splitext(os.path.basename(father[0]))[0]\n",
        "    mother_name = os.path.splitext(os.path.basename(mother[0]))[0]\n",
        "    output_path = os.path.join(output_dir, f\"combined_{father_name}_{mother_name}.jpg\")\n",
        "    combined_image.save(output_path)\n",
        "\n",
        "    # Store results\n",
        "    parent_images.append(combined_image)\n",
        "    parent_latents.append(combined_latent)\n",
        "    parent_image_paths.append(output_path)  # Store the path of the combined image\n",
        "\n",
        "    # Display first family\n",
        "    if i == 0:\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(Image.open(father_path))\n",
        "        plt.title(\"Father\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(Image.open(mother_path))\n",
        "        plt.title(\"Mother\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(combined_image)\n",
        "        plt.title(\"Combined Parents\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "print(f\"Generated {len(parent_image_paths)} combined parent images\")"
      ],
      "metadata": {
        "id": "Qt8MfIyO8E13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79ccaec-08d5-48a9-aed5-ec6e516d0e2f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pSp from checkpoint: pretrained_models/psp_ffhq_encode.pt\n",
            "pSp encoder loaded successfully\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "Father latent shape: torch.Size([1, 18, 512])\n",
            "Mother latent shape: torch.Size([1, 18, 512])\n",
            "Generated 10 combined parent images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import facer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def analyze_face(image_path):\n",
        "    \"\"\"Analyze facial features in an image using facer.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image or PIL Image\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (attribute_map, face_crop_path)\n",
        "        attribute_map contains normalized scores for facial features\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load and prepare image\n",
        "        if isinstance(image_path, str):\n",
        "            try:\n",
        "                # Convert image to facer format\n",
        "                image = facer.hwc2bchw(facer.read_hwc(image_path)).to(device=device)\n",
        "                print(f\"Successfully loaded image from {image_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image from {image_path}: {str(e)}\")\n",
        "                return None, None\n",
        "        else:\n",
        "            # Convert PIL Image to facer format\n",
        "            img_path = \"temp_face.jpg\"\n",
        "            image_path.save(img_path)\n",
        "            image = facer.hwc2bchw(facer.read_hwc(img_path)).to(device=device)\n",
        "\n",
        "        # Initialize face detector and attribute analyzer\n",
        "        face_detector = facer.face_detector(\"retinaface/mobilenet\", device=device)\n",
        "        face_attr = facer.face_attr(\"farl/celeba/224\", device=device)\n",
        "\n",
        "        # Detect faces\n",
        "        with torch.inference_mode():\n",
        "            faces = face_detector(image)\n",
        "            if len(faces[\"rects\"]) == 0:\n",
        "                print(f\"No faces detected in image {image_path}\")\n",
        "                return None, None\n",
        "\n",
        "            # Get face attributes\n",
        "            faces = face_attr(image, faces)\n",
        "\n",
        "            if \"attrs\" not in faces or len(faces[\"attrs\"]) == 0:\n",
        "                print(f\"Could not extract attributes from face in {image_path}\")\n",
        "                return None, None\n",
        "\n",
        "            # Get attributes of first face\n",
        "            face1_attrs = faces[\"attrs\"][0]\n",
        "            labels = face_attr.labels\n",
        "\n",
        "            # Create attribute map\n",
        "            attribute_map = {}\n",
        "            for prob, label in zip(face1_attrs, labels):\n",
        "                attribute_map[label] = prob.item()\n",
        "\n",
        "            print(\"\\nRaw Attribute Map:\")\n",
        "            for label, value in attribute_map.items():\n",
        "                print(f\"{label}: {value:.3f}\")\n",
        "\n",
        "            # Map facer attributes to our required attributes\n",
        "            final_attribute_map = {\n",
        "                'Male': attribute_map.get('Male', 0.0),\n",
        "                'Young': 1 - attribute_map.get('Aged', 0.0),  # Invert 'Aged'\n",
        "                'Smiling': attribute_map.get('Smiling', 0.0),\n",
        "                'Eyeglasses': attribute_map.get('Eyeglasses', 0.0),\n",
        "                'Heavy_Makeup': attribute_map.get('Heavy_Makeup', 0.0),\n",
        "                'Wearing_Hat': attribute_map.get('Wearing_Hat', 0.0),\n",
        "                'Oval_Face': attribute_map.get('Oval_Face', 0.0),\n",
        "                'Pointy_Nose': attribute_map.get('Pointy_Nose', 0.0),\n",
        "                'Mouth_Slightly_Open': attribute_map.get('Mouth_Slightly_Open', 0.0),\n",
        "                'Wearing_Earrings': attribute_map.get('Wearing_Earrings', 0.0),\n",
        "                'Pale_Skin': attribute_map.get('Pale_Skin', 0.0),\n",
        "                '5_o_Clock_Shadow': attribute_map.get('5_o_Clock_Shadow', 0.0),\n",
        "                'Chubby': attribute_map.get('Chubby', 0.0),\n",
        "                'Double_Chin': attribute_map.get('Double_Chin', 0.0),\n",
        "                'Goatee': attribute_map.get('Goatee', 0.0),\n",
        "                'Gray_Hair': attribute_map.get('Gray_Hair', 0.0),\n",
        "                'Mustache': attribute_map.get('Mustache', 0.0),\n",
        "                'No_Beard': attribute_map.get('No_Beard', 1.0),\n",
        "                'Receding_Hairline': attribute_map.get('Receding_Hairline', 0.0),\n",
        "                'Rosy_Cheeks': attribute_map.get('Rosy_Cheeks', 0.0),\n",
        "                'Straight_Hair': attribute_map.get('Straight_Hair', 0.0),\n",
        "                'Wavy_Hair': attribute_map.get('Wavy_Hair', 0.0),\n",
        "                'Wearing_Lipstick': attribute_map.get('Wearing_Lipstick', 0.0)\n",
        "            }\n",
        "\n",
        "            print(\"\\nMapped Attribute Values:\")\n",
        "            for label, value in final_attribute_map.items():\n",
        "                print(f\"{label}: {value:.3f}\")\n",
        "\n",
        "            # Save cropped face if path provided\n",
        "            if isinstance(image_path, str):\n",
        "                # Get face bounding box\n",
        "                bbox = faces[\"rects\"][0]  # First face\n",
        "                x1, y1, x2, y2 = bbox.tolist()\n",
        "\n",
        "                # Convert back to PIL Image for cropping\n",
        "                #TODO jean why tf are we cropping faces? Where did this idea come from\n",
        "                img = Image.open(image_path)\n",
        "                face_crop = img.crop((x1, y1, x2, y2))\n",
        "                face_crop = face_crop.resize((256, 256))\n",
        "\n",
        "                crop_path = image_path.replace('.', '_face.')\n",
        "                face_crop.save(crop_path)\n",
        "                print(f\"\\nSaved cropped face to {crop_path}\")\n",
        "            else:\n",
        "                crop_path = None\n",
        "\n",
        "            return final_attribute_map, crop_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in analyze_face: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "# Feature extraction\n",
        "parent_labels = []\n",
        "child_labels = []\n",
        "\n",
        "# Iterate through parent image paths\n",
        "print(\"Extracting features from parent images...\")\n",
        "for parent_image_path in parent_image_paths:\n",
        "    attribute_map, _ = analyze_face(parent_image_path)\n",
        "    if attribute_map is not None:  # Only add if face was detected\n",
        "        parent_labels.append((attribute_map, parent_image_path))\n",
        "    print(f\"Processed {parent_image_path}\")\n",
        "\n",
        "print(f\"\\nExtracting features from child images...\")\n",
        "for child in children:\n",
        "    child_path = append_to_base_path(child[0])\n",
        "    attribute_map, _ = analyze_face(child_path)\n",
        "    if attribute_map is not None:  # Only add if face was detected\n",
        "        child_labels.append((attribute_map, child_path))\n",
        "    print(f\"Processed {child_path}\")\n",
        "\n",
        "print(f\"\\nProcessed {len(parent_labels)} parent images and {len(child_labels)} child images\")\n"
      ],
      "metadata": {
        "id": "JfTUNpM_8I5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d04e3fb-510e-4900-9136-58e37d1a0b02"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features from parent images...\n",
            "Successfully loaded image from generated_parents/combined_Barack_Obama_20403_Michelle_Obama_00007.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/elliottzheng/face-detection/releases/download/0.0.1/mobilenet0.25_Final.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet0.25_Final.pth\n",
            "100%|██████████| 1.71M/1.71M [00:00<00:00, 36.0MB/s]\n",
            "Downloading: \"https://github.com/FacePerceiver/facer/releases/download/models-v1/face_attribute.farl.celeba.pt\" to /root/.cache/torch/hub/checkpoints/face_attribute.farl.celeba.pt\n",
            "100%|██████████| 327M/327M [00:09<00:00, 36.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.003\n",
            "Arched_Eyebrows: 0.191\n",
            "Attractive: 0.013\n",
            "Bags_Under_Eyes: 0.539\n",
            "Bald: 0.006\n",
            "Bangs: 0.001\n",
            "Big_Lips: 0.126\n",
            "Big_Nose: 0.847\n",
            "Black_Hair: 0.088\n",
            "Blond_Hair: 0.001\n",
            "Blurry: 0.023\n",
            "Brown_Hair: 0.069\n",
            "Bushy_Eyebrows: 0.926\n",
            "Chubby: 0.449\n",
            "Double_Chin: 0.198\n",
            "Eyeglasses: 0.003\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Heavy_Makeup: 0.004\n",
            "High_Cheekbones: 0.953\n",
            "Male: 0.544\n",
            "Mouth_Slightly_Open: 0.437\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.565\n",
            "No_Beard: 0.990\n",
            "Oval_Face: 0.371\n",
            "Pale_Skin: 0.000\n",
            "Pointy_Nose: 0.001\n",
            "Receding_Hairline: 0.920\n",
            "Rosy_Cheeks: 0.000\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.988\n",
            "Straight_Hair: 0.071\n",
            "Wavy_Hair: 0.016\n",
            "Wearing_Earrings: 0.022\n",
            "Wearing_Hat: 0.000\n",
            "Wearing_Lipstick: 0.026\n",
            "Wearing_Necklace: 0.020\n",
            "Wearing_Necktie: 0.004\n",
            "Young: 0.575\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.544\n",
            "Young: 1.000\n",
            "Smiling: 0.988\n",
            "Eyeglasses: 0.003\n",
            "Heavy_Makeup: 0.004\n",
            "Wearing_Hat: 0.000\n",
            "Oval_Face: 0.371\n",
            "Pointy_Nose: 0.001\n",
            "Mouth_Slightly_Open: 0.437\n",
            "Wearing_Earrings: 0.022\n",
            "Pale_Skin: 0.000\n",
            "5_o_Clock_Shadow: 0.003\n",
            "Chubby: 0.449\n",
            "Double_Chin: 0.198\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.990\n",
            "Receding_Hairline: 0.920\n",
            "Rosy_Cheeks: 0.000\n",
            "Straight_Hair: 0.071\n",
            "Wavy_Hair: 0.016\n",
            "Wearing_Lipstick: 0.026\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Barack_Obama_20403_Michelle_Obama_00007_face.jpg\n",
            "Processed generated_parents/combined_Barack_Obama_20403_Michelle_Obama_00007.jpg\n",
            "Successfully loaded image from generated_parents/combined_Barack_Obama_20091_Michelle_Obama_00015.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.016\n",
            "Arched_Eyebrows: 0.161\n",
            "Attractive: 0.003\n",
            "Bags_Under_Eyes: 0.647\n",
            "Bald: 0.007\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.041\n",
            "Big_Nose: 0.400\n",
            "Black_Hair: 0.774\n",
            "Blond_Hair: 0.001\n",
            "Blurry: 0.033\n",
            "Brown_Hair: 0.006\n",
            "Bushy_Eyebrows: 0.947\n",
            "Chubby: 0.845\n",
            "Double_Chin: 0.372\n",
            "Eyeglasses: 0.004\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Heavy_Makeup: 0.002\n",
            "High_Cheekbones: 0.987\n",
            "Male: 0.957\n",
            "Mouth_Slightly_Open: 0.939\n",
            "Mustache: 0.002\n",
            "Narrow_Eyes: 0.504\n",
            "No_Beard: 0.941\n",
            "Oval_Face: 0.511\n",
            "Pale_Skin: 0.000\n",
            "Pointy_Nose: 0.002\n",
            "Receding_Hairline: 0.755\n",
            "Rosy_Cheeks: 0.001\n",
            "Sideburns: 0.002\n",
            "Smiling: 0.989\n",
            "Straight_Hair: 0.099\n",
            "Wavy_Hair: 0.019\n",
            "Wearing_Earrings: 0.012\n",
            "Wearing_Hat: 0.000\n",
            "Wearing_Lipstick: 0.005\n",
            "Wearing_Necklace: 0.004\n",
            "Wearing_Necktie: 0.003\n",
            "Young: 0.431\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.957\n",
            "Young: 1.000\n",
            "Smiling: 0.989\n",
            "Eyeglasses: 0.004\n",
            "Heavy_Makeup: 0.002\n",
            "Wearing_Hat: 0.000\n",
            "Oval_Face: 0.511\n",
            "Pointy_Nose: 0.002\n",
            "Mouth_Slightly_Open: 0.939\n",
            "Wearing_Earrings: 0.012\n",
            "Pale_Skin: 0.000\n",
            "5_o_Clock_Shadow: 0.016\n",
            "Chubby: 0.845\n",
            "Double_Chin: 0.372\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Mustache: 0.002\n",
            "No_Beard: 0.941\n",
            "Receding_Hairline: 0.755\n",
            "Rosy_Cheeks: 0.001\n",
            "Straight_Hair: 0.099\n",
            "Wavy_Hair: 0.019\n",
            "Wearing_Lipstick: 0.005\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Barack_Obama_20091_Michelle_Obama_00015_face.jpg\n",
            "Processed generated_parents/combined_Barack_Obama_20091_Michelle_Obama_00015.jpg\n",
            "Successfully loaded image from generated_parents/combined_Barack_Obama_Sr_00002_Ann_Dunham_00013.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.021\n",
            "Arched_Eyebrows: 0.098\n",
            "Attractive: 0.003\n",
            "Bags_Under_Eyes: 0.142\n",
            "Bald: 0.001\n",
            "Bangs: 0.001\n",
            "Big_Lips: 0.147\n",
            "Big_Nose: 0.373\n",
            "Black_Hair: 0.045\n",
            "Blond_Hair: 0.001\n",
            "Blurry: 0.268\n",
            "Brown_Hair: 0.237\n",
            "Bushy_Eyebrows: 0.908\n",
            "Chubby: 0.881\n",
            "Double_Chin: 0.459\n",
            "Eyeglasses: 0.675\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.001\n",
            "Heavy_Makeup: 0.003\n",
            "High_Cheekbones: 0.780\n",
            "Male: 0.669\n",
            "Mouth_Slightly_Open: 0.181\n",
            "Mustache: 0.003\n",
            "Narrow_Eyes: 0.243\n",
            "No_Beard: 0.941\n",
            "Oval_Face: 0.210\n",
            "Pale_Skin: 0.004\n",
            "Pointy_Nose: 0.003\n",
            "Receding_Hairline: 0.201\n",
            "Rosy_Cheeks: 0.001\n",
            "Sideburns: 0.003\n",
            "Smiling: 0.545\n",
            "Straight_Hair: 0.023\n",
            "Wavy_Hair: 0.302\n",
            "Wearing_Earrings: 0.009\n",
            "Wearing_Hat: 0.002\n",
            "Wearing_Lipstick: 0.010\n",
            "Wearing_Necklace: 0.010\n",
            "Wearing_Necktie: 0.006\n",
            "Young: 0.348\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.669\n",
            "Young: 1.000\n",
            "Smiling: 0.545\n",
            "Eyeglasses: 0.675\n",
            "Heavy_Makeup: 0.003\n",
            "Wearing_Hat: 0.002\n",
            "Oval_Face: 0.210\n",
            "Pointy_Nose: 0.003\n",
            "Mouth_Slightly_Open: 0.181\n",
            "Wearing_Earrings: 0.009\n",
            "Pale_Skin: 0.004\n",
            "5_o_Clock_Shadow: 0.021\n",
            "Chubby: 0.881\n",
            "Double_Chin: 0.459\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.001\n",
            "Mustache: 0.003\n",
            "No_Beard: 0.941\n",
            "Receding_Hairline: 0.201\n",
            "Rosy_Cheeks: 0.001\n",
            "Straight_Hair: 0.023\n",
            "Wavy_Hair: 0.302\n",
            "Wearing_Lipstick: 0.010\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Barack_Obama_Sr_00002_Ann_Dunham_00013_face.jpg\n",
            "Processed generated_parents/combined_Barack_Obama_Sr_00002_Ann_Dunham_00013.jpg\n",
            "Successfully loaded image from generated_parents/combined_Lolo_Soetoro_00001_Ann_Dunham_00005.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.005\n",
            "Arched_Eyebrows: 0.092\n",
            "Attractive: 0.015\n",
            "Bags_Under_Eyes: 0.145\n",
            "Bald: 0.002\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.018\n",
            "Big_Nose: 0.131\n",
            "Black_Hair: 0.928\n",
            "Blond_Hair: 0.000\n",
            "Blurry: 0.128\n",
            "Brown_Hair: 0.003\n",
            "Bushy_Eyebrows: 0.230\n",
            "Chubby: 0.827\n",
            "Double_Chin: 0.304\n",
            "Eyeglasses: 0.021\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.002\n",
            "Heavy_Makeup: 0.003\n",
            "High_Cheekbones: 0.754\n",
            "Male: 0.837\n",
            "Mouth_Slightly_Open: 0.026\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.235\n",
            "No_Beard: 0.990\n",
            "Oval_Face: 0.349\n",
            "Pale_Skin: 0.096\n",
            "Pointy_Nose: 0.007\n",
            "Receding_Hairline: 0.370\n",
            "Rosy_Cheeks: 0.000\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.656\n",
            "Straight_Hair: 0.278\n",
            "Wavy_Hair: 0.014\n",
            "Wearing_Earrings: 0.005\n",
            "Wearing_Hat: 0.000\n",
            "Wearing_Lipstick: 0.005\n",
            "Wearing_Necklace: 0.004\n",
            "Wearing_Necktie: 0.005\n",
            "Young: 0.381\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.837\n",
            "Young: 1.000\n",
            "Smiling: 0.656\n",
            "Eyeglasses: 0.021\n",
            "Heavy_Makeup: 0.003\n",
            "Wearing_Hat: 0.000\n",
            "Oval_Face: 0.349\n",
            "Pointy_Nose: 0.007\n",
            "Mouth_Slightly_Open: 0.026\n",
            "Wearing_Earrings: 0.005\n",
            "Pale_Skin: 0.096\n",
            "5_o_Clock_Shadow: 0.005\n",
            "Chubby: 0.827\n",
            "Double_Chin: 0.304\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.002\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.990\n",
            "Receding_Hairline: 0.370\n",
            "Rosy_Cheeks: 0.000\n",
            "Straight_Hair: 0.278\n",
            "Wavy_Hair: 0.014\n",
            "Wearing_Lipstick: 0.005\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Lolo_Soetoro_00001_Ann_Dunham_00005_face.jpg\n",
            "Processed generated_parents/combined_Lolo_Soetoro_00001_Ann_Dunham_00005.jpg\n",
            "Successfully loaded image from generated_parents/combined_Stanley_Dunham_00002_Madelyn_Payne_00002.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.005\n",
            "Arched_Eyebrows: 0.205\n",
            "Attractive: 0.152\n",
            "Bags_Under_Eyes: 0.135\n",
            "Bald: 0.000\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.181\n",
            "Big_Nose: 0.154\n",
            "Black_Hair: 0.162\n",
            "Blond_Hair: 0.001\n",
            "Blurry: 0.094\n",
            "Brown_Hair: 0.062\n",
            "Bushy_Eyebrows: 0.507\n",
            "Chubby: 0.065\n",
            "Double_Chin: 0.013\n",
            "Eyeglasses: 0.005\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Heavy_Makeup: 0.010\n",
            "High_Cheekbones: 0.311\n",
            "Male: 0.492\n",
            "Mouth_Slightly_Open: 0.927\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.326\n",
            "No_Beard: 0.988\n",
            "Oval_Face: 0.281\n",
            "Pale_Skin: 0.853\n",
            "Pointy_Nose: 0.006\n",
            "Receding_Hairline: 0.058\n",
            "Rosy_Cheeks: 0.000\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.838\n",
            "Straight_Hair: 0.208\n",
            "Wavy_Hair: 0.023\n",
            "Wearing_Earrings: 0.006\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.023\n",
            "Wearing_Necklace: 0.004\n",
            "Wearing_Necktie: 0.001\n",
            "Young: 0.890\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.492\n",
            "Young: 1.000\n",
            "Smiling: 0.838\n",
            "Eyeglasses: 0.005\n",
            "Heavy_Makeup: 0.010\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.281\n",
            "Pointy_Nose: 0.006\n",
            "Mouth_Slightly_Open: 0.927\n",
            "Wearing_Earrings: 0.006\n",
            "Pale_Skin: 0.853\n",
            "5_o_Clock_Shadow: 0.005\n",
            "Chubby: 0.065\n",
            "Double_Chin: 0.013\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.988\n",
            "Receding_Hairline: 0.058\n",
            "Rosy_Cheeks: 0.000\n",
            "Straight_Hair: 0.208\n",
            "Wavy_Hair: 0.023\n",
            "Wearing_Lipstick: 0.023\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Stanley_Dunham_00002_Madelyn_Payne_00002_face.jpg\n",
            "Processed generated_parents/combined_Stanley_Dunham_00002_Madelyn_Payne_00002.jpg\n",
            "Successfully loaded image from generated_parents/combined_Jeb_Bush_0030_Columba_Bush_0001.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.019\n",
            "Arched_Eyebrows: 0.010\n",
            "Attractive: 0.025\n",
            "Bags_Under_Eyes: 0.209\n",
            "Bald: 0.001\n",
            "Bangs: 0.001\n",
            "Big_Lips: 0.067\n",
            "Big_Nose: 0.051\n",
            "Black_Hair: 0.001\n",
            "Blond_Hair: 0.071\n",
            "Blurry: 0.027\n",
            "Brown_Hair: 0.170\n",
            "Bushy_Eyebrows: 0.579\n",
            "Chubby: 0.062\n",
            "Double_Chin: 0.012\n",
            "Eyeglasses: 0.001\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.003\n",
            "Heavy_Makeup: 0.001\n",
            "High_Cheekbones: 0.812\n",
            "Male: 0.983\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.854\n",
            "No_Beard: 0.989\n",
            "Oval_Face: 0.255\n",
            "Pale_Skin: 0.002\n",
            "Pointy_Nose: 0.007\n",
            "Receding_Hairline: 0.011\n",
            "Rosy_Cheeks: 0.001\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.900\n",
            "Straight_Hair: 0.107\n",
            "Wavy_Hair: 0.036\n",
            "Wearing_Earrings: 0.004\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.002\n",
            "Wearing_Necklace: 0.004\n",
            "Wearing_Necktie: 0.001\n",
            "Young: 0.856\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.983\n",
            "Young: 1.000\n",
            "Smiling: 0.900\n",
            "Eyeglasses: 0.001\n",
            "Heavy_Makeup: 0.001\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.255\n",
            "Pointy_Nose: 0.007\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Wearing_Earrings: 0.004\n",
            "Pale_Skin: 0.002\n",
            "5_o_Clock_Shadow: 0.019\n",
            "Chubby: 0.062\n",
            "Double_Chin: 0.012\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.003\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.989\n",
            "Receding_Hairline: 0.011\n",
            "Rosy_Cheeks: 0.001\n",
            "Straight_Hair: 0.107\n",
            "Wavy_Hair: 0.036\n",
            "Wearing_Lipstick: 0.002\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Jeb_Bush_0030_Columba_Bush_0001_face.jpg\n",
            "Processed generated_parents/combined_Jeb_Bush_0030_Columba_Bush_0001.jpg\n",
            "Successfully loaded image from generated_parents/combined_Bill_Clinton_0029_Hillary_Clinton_0005.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.002\n",
            "Arched_Eyebrows: 0.125\n",
            "Attractive: 0.003\n",
            "Bags_Under_Eyes: 0.077\n",
            "Bald: 0.020\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.116\n",
            "Big_Nose: 0.395\n",
            "Black_Hair: 0.004\n",
            "Blond_Hair: 0.083\n",
            "Blurry: 0.033\n",
            "Brown_Hair: 0.001\n",
            "Bushy_Eyebrows: 0.020\n",
            "Chubby: 0.825\n",
            "Double_Chin: 0.235\n",
            "Eyeglasses: 0.001\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.275\n",
            "Heavy_Makeup: 0.006\n",
            "High_Cheekbones: 0.511\n",
            "Male: 0.958\n",
            "Mouth_Slightly_Open: 0.995\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.301\n",
            "No_Beard: 0.982\n",
            "Oval_Face: 0.394\n",
            "Pale_Skin: 0.003\n",
            "Pointy_Nose: 0.005\n",
            "Receding_Hairline: 0.587\n",
            "Rosy_Cheeks: 0.001\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.601\n",
            "Straight_Hair: 0.082\n",
            "Wavy_Hair: 0.011\n",
            "Wearing_Earrings: 0.008\n",
            "Wearing_Hat: 0.000\n",
            "Wearing_Lipstick: 0.042\n",
            "Wearing_Necklace: 0.006\n",
            "Wearing_Necktie: 0.002\n",
            "Young: 0.207\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.958\n",
            "Young: 1.000\n",
            "Smiling: 0.601\n",
            "Eyeglasses: 0.001\n",
            "Heavy_Makeup: 0.006\n",
            "Wearing_Hat: 0.000\n",
            "Oval_Face: 0.394\n",
            "Pointy_Nose: 0.005\n",
            "Mouth_Slightly_Open: 0.995\n",
            "Wearing_Earrings: 0.008\n",
            "Pale_Skin: 0.003\n",
            "5_o_Clock_Shadow: 0.002\n",
            "Chubby: 0.825\n",
            "Double_Chin: 0.235\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.275\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.982\n",
            "Receding_Hairline: 0.587\n",
            "Rosy_Cheeks: 0.001\n",
            "Straight_Hair: 0.082\n",
            "Wavy_Hair: 0.011\n",
            "Wearing_Lipstick: 0.042\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Bill_Clinton_0029_Hillary_Clinton_0005_face.jpg\n",
            "Processed generated_parents/combined_Bill_Clinton_0029_Hillary_Clinton_0005.jpg\n",
            "Successfully loaded image from generated_parents/combined_William_Jefferson_Blythe_0001_Virginia_Clinton_Kelley_0004.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.002\n",
            "Arched_Eyebrows: 0.450\n",
            "Attractive: 0.015\n",
            "Bags_Under_Eyes: 0.407\n",
            "Bald: 0.000\n",
            "Bangs: 0.004\n",
            "Big_Lips: 0.119\n",
            "Big_Nose: 0.417\n",
            "Black_Hair: 0.006\n",
            "Blond_Hair: 0.002\n",
            "Blurry: 0.697\n",
            "Brown_Hair: 0.150\n",
            "Bushy_Eyebrows: 0.095\n",
            "Chubby: 0.211\n",
            "Double_Chin: 0.183\n",
            "Eyeglasses: 0.002\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.004\n",
            "Heavy_Makeup: 0.003\n",
            "High_Cheekbones: 0.922\n",
            "Male: 0.353\n",
            "Mouth_Slightly_Open: 0.985\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.314\n",
            "No_Beard: 0.991\n",
            "Oval_Face: 0.235\n",
            "Pale_Skin: 0.027\n",
            "Pointy_Nose: 0.010\n",
            "Receding_Hairline: 0.006\n",
            "Rosy_Cheeks: 0.000\n",
            "Sideburns: 0.000\n",
            "Smiling: 0.998\n",
            "Straight_Hair: 0.060\n",
            "Wavy_Hair: 0.089\n",
            "Wearing_Earrings: 0.012\n",
            "Wearing_Hat: 0.006\n",
            "Wearing_Lipstick: 0.036\n",
            "Wearing_Necklace: 0.013\n",
            "Wearing_Necktie: 0.002\n",
            "Young: 0.221\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.353\n",
            "Young: 1.000\n",
            "Smiling: 0.998\n",
            "Eyeglasses: 0.002\n",
            "Heavy_Makeup: 0.003\n",
            "Wearing_Hat: 0.006\n",
            "Oval_Face: 0.235\n",
            "Pointy_Nose: 0.010\n",
            "Mouth_Slightly_Open: 0.985\n",
            "Wearing_Earrings: 0.012\n",
            "Pale_Skin: 0.027\n",
            "5_o_Clock_Shadow: 0.002\n",
            "Chubby: 0.211\n",
            "Double_Chin: 0.183\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.004\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.991\n",
            "Receding_Hairline: 0.006\n",
            "Rosy_Cheeks: 0.000\n",
            "Straight_Hair: 0.060\n",
            "Wavy_Hair: 0.089\n",
            "Wearing_Lipstick: 0.036\n",
            "\n",
            "Saved cropped face to generated_parents/combined_William_Jefferson_Blythe_0001_Virginia_Clinton_Kelley_0004_face.jpg\n",
            "Processed generated_parents/combined_William_Jefferson_Blythe_0001_Virginia_Clinton_Kelley_0004.jpg\n",
            "Successfully loaded image from generated_parents/combined_Ronald_Reagan_0023_Jane_Wyman_0020.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.015\n",
            "Arched_Eyebrows: 0.155\n",
            "Attractive: 0.016\n",
            "Bags_Under_Eyes: 0.842\n",
            "Bald: 0.003\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.082\n",
            "Big_Nose: 0.507\n",
            "Black_Hair: 0.006\n",
            "Blond_Hair: 0.004\n",
            "Blurry: 0.292\n",
            "Brown_Hair: 0.029\n",
            "Bushy_Eyebrows: 0.646\n",
            "Chubby: 0.183\n",
            "Double_Chin: 0.132\n",
            "Eyeglasses: 0.001\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.037\n",
            "Heavy_Makeup: 0.000\n",
            "High_Cheekbones: 0.807\n",
            "Male: 0.991\n",
            "Mouth_Slightly_Open: 0.959\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.475\n",
            "No_Beard: 0.983\n",
            "Oval_Face: 0.144\n",
            "Pale_Skin: 0.017\n",
            "Pointy_Nose: 0.007\n",
            "Receding_Hairline: 0.518\n",
            "Rosy_Cheeks: 0.000\n",
            "Sideburns: 0.002\n",
            "Smiling: 0.987\n",
            "Straight_Hair: 0.096\n",
            "Wavy_Hair: 0.028\n",
            "Wearing_Earrings: 0.002\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.001\n",
            "Wearing_Necklace: 0.004\n",
            "Wearing_Necktie: 0.009\n",
            "Young: 0.250\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.991\n",
            "Young: 1.000\n",
            "Smiling: 0.987\n",
            "Eyeglasses: 0.001\n",
            "Heavy_Makeup: 0.000\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.144\n",
            "Pointy_Nose: 0.007\n",
            "Mouth_Slightly_Open: 0.959\n",
            "Wearing_Earrings: 0.002\n",
            "Pale_Skin: 0.017\n",
            "5_o_Clock_Shadow: 0.015\n",
            "Chubby: 0.183\n",
            "Double_Chin: 0.132\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.037\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.983\n",
            "Receding_Hairline: 0.518\n",
            "Rosy_Cheeks: 0.000\n",
            "Straight_Hair: 0.096\n",
            "Wavy_Hair: 0.028\n",
            "Wearing_Lipstick: 0.001\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Ronald_Reagan_0023_Jane_Wyman_0020_face.jpg\n",
            "Processed generated_parents/combined_Ronald_Reagan_0023_Jane_Wyman_0020.jpg\n",
            "Successfully loaded image from generated_parents/combined_Ronald_Reagan_0012_Nancy_Reagan_0014.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.053\n",
            "Arched_Eyebrows: 0.498\n",
            "Attractive: 0.022\n",
            "Bags_Under_Eyes: 0.911\n",
            "Bald: 0.002\n",
            "Bangs: 0.001\n",
            "Big_Lips: 0.117\n",
            "Big_Nose: 0.651\n",
            "Black_Hair: 0.124\n",
            "Blond_Hair: 0.001\n",
            "Blurry: 0.031\n",
            "Brown_Hair: 0.261\n",
            "Bushy_Eyebrows: 0.967\n",
            "Chubby: 0.352\n",
            "Double_Chin: 0.490\n",
            "Eyeglasses: 0.002\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Heavy_Makeup: 0.001\n",
            "High_Cheekbones: 0.983\n",
            "Male: 0.996\n",
            "Mouth_Slightly_Open: 0.996\n",
            "Mustache: 0.002\n",
            "Narrow_Eyes: 0.937\n",
            "No_Beard: 0.985\n",
            "Oval_Face: 0.085\n",
            "Pale_Skin: 0.001\n",
            "Pointy_Nose: 0.021\n",
            "Receding_Hairline: 0.067\n",
            "Rosy_Cheeks: 0.002\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.999\n",
            "Straight_Hair: 0.197\n",
            "Wavy_Hair: 0.111\n",
            "Wearing_Earrings: 0.005\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.004\n",
            "Wearing_Necklace: 0.007\n",
            "Wearing_Necktie: 0.023\n",
            "Young: 0.161\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.996\n",
            "Young: 1.000\n",
            "Smiling: 0.999\n",
            "Eyeglasses: 0.002\n",
            "Heavy_Makeup: 0.001\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.085\n",
            "Pointy_Nose: 0.021\n",
            "Mouth_Slightly_Open: 0.996\n",
            "Wearing_Earrings: 0.005\n",
            "Pale_Skin: 0.001\n",
            "5_o_Clock_Shadow: 0.053\n",
            "Chubby: 0.352\n",
            "Double_Chin: 0.490\n",
            "Goatee: 0.001\n",
            "Gray_Hair: 0.002\n",
            "Mustache: 0.002\n",
            "No_Beard: 0.985\n",
            "Receding_Hairline: 0.067\n",
            "Rosy_Cheeks: 0.002\n",
            "Straight_Hair: 0.197\n",
            "Wavy_Hair: 0.111\n",
            "Wearing_Lipstick: 0.004\n",
            "\n",
            "Saved cropped face to generated_parents/combined_Ronald_Reagan_0012_Nancy_Reagan_0014_face.jpg\n",
            "Processed generated_parents/combined_Ronald_Reagan_0012_Nancy_Reagan_0014.jpg\n",
            "\n",
            "Extracting features from child images...\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Obama/Malia_Ann_Obama/Malia_Ann_Obama_00013.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.004\n",
            "Arched_Eyebrows: 0.272\n",
            "Attractive: 0.039\n",
            "Bags_Under_Eyes: 0.034\n",
            "Bald: 0.000\n",
            "Bangs: 0.002\n",
            "Big_Lips: 0.282\n",
            "Big_Nose: 0.056\n",
            "Black_Hair: 0.038\n",
            "Blond_Hair: 0.005\n",
            "Blurry: 0.893\n",
            "Brown_Hair: 0.420\n",
            "Bushy_Eyebrows: 0.061\n",
            "Chubby: 0.007\n",
            "Double_Chin: 0.001\n",
            "Eyeglasses: 0.002\n",
            "Goatee: 0.002\n",
            "Gray_Hair: 0.000\n",
            "Heavy_Makeup: 0.016\n",
            "High_Cheekbones: 0.035\n",
            "Male: 0.019\n",
            "Mouth_Slightly_Open: 0.056\n",
            "Mustache: 0.004\n",
            "Narrow_Eyes: 0.030\n",
            "No_Beard: 0.966\n",
            "Oval_Face: 0.304\n",
            "Pale_Skin: 0.001\n",
            "Pointy_Nose: 0.018\n",
            "Receding_Hairline: 0.400\n",
            "Rosy_Cheeks: 0.000\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.059\n",
            "Straight_Hair: 0.003\n",
            "Wavy_Hair: 0.895\n",
            "Wearing_Earrings: 0.005\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.073\n",
            "Wearing_Necklace: 0.025\n",
            "Wearing_Necktie: 0.001\n",
            "Young: 0.949\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.019\n",
            "Young: 1.000\n",
            "Smiling: 0.059\n",
            "Eyeglasses: 0.002\n",
            "Heavy_Makeup: 0.016\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.304\n",
            "Pointy_Nose: 0.018\n",
            "Mouth_Slightly_Open: 0.056\n",
            "Wearing_Earrings: 0.005\n",
            "Pale_Skin: 0.001\n",
            "5_o_Clock_Shadow: 0.004\n",
            "Chubby: 0.007\n",
            "Double_Chin: 0.001\n",
            "Goatee: 0.002\n",
            "Gray_Hair: 0.000\n",
            "Mustache: 0.004\n",
            "No_Beard: 0.966\n",
            "Receding_Hairline: 0.400\n",
            "Rosy_Cheeks: 0.000\n",
            "Straight_Hair: 0.003\n",
            "Wavy_Hair: 0.895\n",
            "Wearing_Lipstick: 0.073\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Obama/Malia_Ann_Obama/Malia_Ann_Obama_00013_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Obama/Malia_Ann_Obama/Malia_Ann_Obama_00013.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Obama/Natasha_Obama/Natasha_Obama_00019.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.001\n",
            "Arched_Eyebrows: 0.047\n",
            "Attractive: 0.028\n",
            "Bags_Under_Eyes: 0.486\n",
            "Bald: 0.001\n",
            "Bangs: 0.002\n",
            "Big_Lips: 0.565\n",
            "Big_Nose: 0.621\n",
            "Black_Hair: 0.383\n",
            "Blond_Hair: 0.002\n",
            "Blurry: 0.208\n",
            "Brown_Hair: 0.012\n",
            "Bushy_Eyebrows: 0.007\n",
            "Chubby: 0.039\n",
            "Double_Chin: 0.007\n",
            "Eyeglasses: 0.001\n",
            "Goatee: 0.002\n",
            "Gray_Hair: 0.001\n",
            "Heavy_Makeup: 0.020\n",
            "High_Cheekbones: 0.994\n",
            "Male: 0.015\n",
            "Mouth_Slightly_Open: 0.995\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.064\n",
            "No_Beard: 0.992\n",
            "Oval_Face: 0.249\n",
            "Pale_Skin: 0.001\n",
            "Pointy_Nose: 0.004\n",
            "Receding_Hairline: 0.878\n",
            "Rosy_Cheeks: 0.001\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.999\n",
            "Straight_Hair: 0.007\n",
            "Wavy_Hair: 0.226\n",
            "Wearing_Earrings: 0.036\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.097\n",
            "Wearing_Necklace: 0.026\n",
            "Wearing_Necktie: 0.002\n",
            "Young: 0.970\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.015\n",
            "Young: 1.000\n",
            "Smiling: 0.999\n",
            "Eyeglasses: 0.001\n",
            "Heavy_Makeup: 0.020\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.249\n",
            "Pointy_Nose: 0.004\n",
            "Mouth_Slightly_Open: 0.995\n",
            "Wearing_Earrings: 0.036\n",
            "Pale_Skin: 0.001\n",
            "5_o_Clock_Shadow: 0.001\n",
            "Chubby: 0.039\n",
            "Double_Chin: 0.007\n",
            "Goatee: 0.002\n",
            "Gray_Hair: 0.001\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.992\n",
            "Receding_Hairline: 0.878\n",
            "Rosy_Cheeks: 0.001\n",
            "Straight_Hair: 0.007\n",
            "Wavy_Hair: 0.226\n",
            "Wearing_Lipstick: 0.097\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Obama/Natasha_Obama/Natasha_Obama_00019_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Obama/Natasha_Obama/Natasha_Obama_00019.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Obama/Barack_Obama/Barack_Obama_20304.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.049\n",
            "Arched_Eyebrows: 0.004\n",
            "Attractive: 0.070\n",
            "Bags_Under_Eyes: 0.862\n",
            "Bald: 0.003\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.081\n",
            "Big_Nose: 0.783\n",
            "Black_Hair: 0.334\n",
            "Blond_Hair: 0.000\n",
            "Blurry: 0.139\n",
            "Brown_Hair: 0.007\n",
            "Bushy_Eyebrows: 0.702\n",
            "Chubby: 0.025\n",
            "Double_Chin: 0.027\n",
            "Eyeglasses: 0.001\n",
            "Goatee: 0.011\n",
            "Gray_Hair: 0.002\n",
            "Heavy_Makeup: 0.000\n",
            "High_Cheekbones: 0.926\n",
            "Male: 0.998\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Mustache: 0.008\n",
            "Narrow_Eyes: 0.927\n",
            "No_Beard: 0.903\n",
            "Oval_Face: 0.291\n",
            "Pale_Skin: 0.000\n",
            "Pointy_Nose: 0.004\n",
            "Receding_Hairline: 0.035\n",
            "Rosy_Cheeks: 0.000\n",
            "Sideburns: 0.002\n",
            "Smiling: 0.998\n",
            "Straight_Hair: 0.116\n",
            "Wavy_Hair: 0.007\n",
            "Wearing_Earrings: 0.006\n",
            "Wearing_Hat: 0.002\n",
            "Wearing_Lipstick: 0.000\n",
            "Wearing_Necklace: 0.001\n",
            "Wearing_Necktie: 0.021\n",
            "Young: 0.686\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.998\n",
            "Young: 1.000\n",
            "Smiling: 0.998\n",
            "Eyeglasses: 0.001\n",
            "Heavy_Makeup: 0.000\n",
            "Wearing_Hat: 0.002\n",
            "Oval_Face: 0.291\n",
            "Pointy_Nose: 0.004\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Wearing_Earrings: 0.006\n",
            "Pale_Skin: 0.000\n",
            "5_o_Clock_Shadow: 0.049\n",
            "Chubby: 0.025\n",
            "Double_Chin: 0.027\n",
            "Goatee: 0.011\n",
            "Gray_Hair: 0.002\n",
            "Mustache: 0.008\n",
            "No_Beard: 0.903\n",
            "Receding_Hairline: 0.035\n",
            "Rosy_Cheeks: 0.000\n",
            "Straight_Hair: 0.116\n",
            "Wavy_Hair: 0.007\n",
            "Wearing_Lipstick: 0.000\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Obama/Barack_Obama/Barack_Obama_20304_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Obama/Barack_Obama/Barack_Obama_20304.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Obama/Maya_Soetoro/Maya_Soetoro_00021.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.000\n",
            "Arched_Eyebrows: 0.820\n",
            "Attractive: 0.417\n",
            "Bags_Under_Eyes: 0.064\n",
            "Bald: 0.000\n",
            "Bangs: 0.006\n",
            "Big_Lips: 0.366\n",
            "Big_Nose: 0.120\n",
            "Black_Hair: 0.678\n",
            "Blond_Hair: 0.001\n",
            "Blurry: 0.006\n",
            "Brown_Hair: 0.008\n",
            "Bushy_Eyebrows: 0.034\n",
            "Chubby: 0.010\n",
            "Double_Chin: 0.004\n",
            "Eyeglasses: 0.000\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.000\n",
            "Heavy_Makeup: 0.909\n",
            "High_Cheekbones: 0.851\n",
            "Male: 0.000\n",
            "Mouth_Slightly_Open: 0.004\n",
            "Mustache: 0.000\n",
            "Narrow_Eyes: 0.001\n",
            "No_Beard: 1.000\n",
            "Oval_Face: 0.203\n",
            "Pale_Skin: 0.000\n",
            "Pointy_Nose: 0.076\n",
            "Receding_Hairline: 0.002\n",
            "Rosy_Cheeks: 0.010\n",
            "Sideburns: 0.000\n",
            "Smiling: 0.590\n",
            "Straight_Hair: 0.010\n",
            "Wavy_Hair: 0.776\n",
            "Wearing_Earrings: 0.062\n",
            "Wearing_Hat: 0.000\n",
            "Wearing_Lipstick: 0.996\n",
            "Wearing_Necklace: 0.070\n",
            "Wearing_Necktie: 0.000\n",
            "Young: 0.780\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.000\n",
            "Young: 1.000\n",
            "Smiling: 0.590\n",
            "Eyeglasses: 0.000\n",
            "Heavy_Makeup: 0.909\n",
            "Wearing_Hat: 0.000\n",
            "Oval_Face: 0.203\n",
            "Pointy_Nose: 0.076\n",
            "Mouth_Slightly_Open: 0.004\n",
            "Wearing_Earrings: 0.062\n",
            "Pale_Skin: 0.000\n",
            "5_o_Clock_Shadow: 0.000\n",
            "Chubby: 0.010\n",
            "Double_Chin: 0.004\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.000\n",
            "Mustache: 0.000\n",
            "No_Beard: 1.000\n",
            "Receding_Hairline: 0.002\n",
            "Rosy_Cheeks: 0.010\n",
            "Straight_Hair: 0.010\n",
            "Wavy_Hair: 0.776\n",
            "Wearing_Lipstick: 0.996\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Obama/Maya_Soetoro/Maya_Soetoro_00021_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Obama/Maya_Soetoro/Maya_Soetoro_00021.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Obama/Ann_Dunham/Ann_Dunham_00006.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.002\n",
            "Arched_Eyebrows: 0.407\n",
            "Attractive: 0.046\n",
            "Bags_Under_Eyes: 0.009\n",
            "Bald: 0.000\n",
            "Bangs: 0.001\n",
            "Big_Lips: 0.037\n",
            "Big_Nose: 0.040\n",
            "Black_Hair: 0.550\n",
            "Blond_Hair: 0.001\n",
            "Blurry: 0.935\n",
            "Brown_Hair: 0.018\n",
            "Bushy_Eyebrows: 0.013\n",
            "Chubby: 0.148\n",
            "Double_Chin: 0.092\n",
            "Eyeglasses: 0.003\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.004\n",
            "Heavy_Makeup: 0.053\n",
            "High_Cheekbones: 0.111\n",
            "Male: 0.047\n",
            "Mouth_Slightly_Open: 0.981\n",
            "Mustache: 0.001\n",
            "Narrow_Eyes: 0.088\n",
            "No_Beard: 0.985\n",
            "Oval_Face: 0.496\n",
            "Pale_Skin: 0.075\n",
            "Pointy_Nose: 0.156\n",
            "Receding_Hairline: 0.032\n",
            "Rosy_Cheeks: 0.002\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.548\n",
            "Straight_Hair: 0.081\n",
            "Wavy_Hair: 0.131\n",
            "Wearing_Earrings: 0.256\n",
            "Wearing_Hat: 0.006\n",
            "Wearing_Lipstick: 0.147\n",
            "Wearing_Necklace: 0.025\n",
            "Wearing_Necktie: 0.002\n",
            "Young: 0.259\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.047\n",
            "Young: 1.000\n",
            "Smiling: 0.548\n",
            "Eyeglasses: 0.003\n",
            "Heavy_Makeup: 0.053\n",
            "Wearing_Hat: 0.006\n",
            "Oval_Face: 0.496\n",
            "Pointy_Nose: 0.156\n",
            "Mouth_Slightly_Open: 0.981\n",
            "Wearing_Earrings: 0.256\n",
            "Pale_Skin: 0.075\n",
            "5_o_Clock_Shadow: 0.002\n",
            "Chubby: 0.148\n",
            "Double_Chin: 0.092\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.004\n",
            "Mustache: 0.001\n",
            "No_Beard: 0.985\n",
            "Receding_Hairline: 0.032\n",
            "Rosy_Cheeks: 0.002\n",
            "Straight_Hair: 0.081\n",
            "Wavy_Hair: 0.131\n",
            "Wearing_Lipstick: 0.147\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Obama/Ann_Dunham/Ann_Dunham_00006_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Obama/Ann_Dunham/Ann_Dunham_00006.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Bush/George_Prescott_Bush/George_Prescott_Bush_0008.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.562\n",
            "Arched_Eyebrows: 0.007\n",
            "Attractive: 0.306\n",
            "Bags_Under_Eyes: 0.149\n",
            "Bald: 0.001\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.159\n",
            "Big_Nose: 0.057\n",
            "Black_Hair: 0.853\n",
            "Blond_Hair: 0.000\n",
            "Blurry: 0.025\n",
            "Brown_Hair: 0.004\n",
            "Bushy_Eyebrows: 0.915\n",
            "Chubby: 0.009\n",
            "Double_Chin: 0.014\n",
            "Eyeglasses: 0.001\n",
            "Goatee: 0.004\n",
            "Gray_Hair: 0.000\n",
            "Heavy_Makeup: 0.001\n",
            "High_Cheekbones: 0.869\n",
            "Male: 0.999\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Mustache: 0.004\n",
            "Narrow_Eyes: 0.017\n",
            "No_Beard: 0.653\n",
            "Oval_Face: 0.265\n",
            "Pale_Skin: 0.001\n",
            "Pointy_Nose: 0.086\n",
            "Receding_Hairline: 0.056\n",
            "Rosy_Cheeks: 0.001\n",
            "Sideburns: 0.006\n",
            "Smiling: 0.997\n",
            "Straight_Hair: 0.291\n",
            "Wavy_Hair: 0.009\n",
            "Wearing_Earrings: 0.002\n",
            "Wearing_Hat: 0.000\n",
            "Wearing_Lipstick: 0.002\n",
            "Wearing_Necklace: 0.001\n",
            "Wearing_Necktie: 0.002\n",
            "Young: 0.656\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.999\n",
            "Young: 1.000\n",
            "Smiling: 0.997\n",
            "Eyeglasses: 0.001\n",
            "Heavy_Makeup: 0.001\n",
            "Wearing_Hat: 0.000\n",
            "Oval_Face: 0.265\n",
            "Pointy_Nose: 0.086\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Wearing_Earrings: 0.002\n",
            "Pale_Skin: 0.001\n",
            "5_o_Clock_Shadow: 0.562\n",
            "Chubby: 0.009\n",
            "Double_Chin: 0.014\n",
            "Goatee: 0.004\n",
            "Gray_Hair: 0.000\n",
            "Mustache: 0.004\n",
            "No_Beard: 0.653\n",
            "Receding_Hairline: 0.056\n",
            "Rosy_Cheeks: 0.001\n",
            "Straight_Hair: 0.291\n",
            "Wavy_Hair: 0.009\n",
            "Wearing_Lipstick: 0.002\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Bush/George_Prescott_Bush/George_Prescott_Bush_0008_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Bush/George_Prescott_Bush/George_Prescott_Bush_0008.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Clinton/Chelsea_Clinton/Chelsea_Clinton_0001.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.000\n",
            "Arched_Eyebrows: 0.917\n",
            "Attractive: 0.831\n",
            "Bags_Under_Eyes: 0.018\n",
            "Bald: 0.000\n",
            "Bangs: 0.000\n",
            "Big_Lips: 0.211\n",
            "Big_Nose: 0.076\n",
            "Black_Hair: 0.001\n",
            "Blond_Hair: 0.564\n",
            "Blurry: 0.006\n",
            "Brown_Hair: 0.056\n",
            "Bushy_Eyebrows: 0.015\n",
            "Chubby: 0.017\n",
            "Double_Chin: 0.018\n",
            "Eyeglasses: 0.000\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.001\n",
            "Heavy_Makeup: 0.971\n",
            "High_Cheekbones: 0.987\n",
            "Male: 0.000\n",
            "Mouth_Slightly_Open: 0.014\n",
            "Mustache: 0.000\n",
            "Narrow_Eyes: 0.021\n",
            "No_Beard: 1.000\n",
            "Oval_Face: 0.443\n",
            "Pale_Skin: 0.336\n",
            "Pointy_Nose: 0.099\n",
            "Receding_Hairline: 0.006\n",
            "Rosy_Cheeks: 0.024\n",
            "Sideburns: 0.000\n",
            "Smiling: 0.982\n",
            "Straight_Hair: 0.366\n",
            "Wavy_Hair: 0.165\n",
            "Wearing_Earrings: 0.150\n",
            "Wearing_Hat: 0.000\n",
            "Wearing_Lipstick: 0.995\n",
            "Wearing_Necklace: 0.025\n",
            "Wearing_Necktie: 0.000\n",
            "Young: 0.868\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.000\n",
            "Young: 1.000\n",
            "Smiling: 0.982\n",
            "Eyeglasses: 0.000\n",
            "Heavy_Makeup: 0.971\n",
            "Wearing_Hat: 0.000\n",
            "Oval_Face: 0.443\n",
            "Pointy_Nose: 0.099\n",
            "Mouth_Slightly_Open: 0.014\n",
            "Wearing_Earrings: 0.150\n",
            "Pale_Skin: 0.336\n",
            "5_o_Clock_Shadow: 0.000\n",
            "Chubby: 0.017\n",
            "Double_Chin: 0.018\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.001\n",
            "Mustache: 0.000\n",
            "No_Beard: 1.000\n",
            "Receding_Hairline: 0.006\n",
            "Rosy_Cheeks: 0.024\n",
            "Straight_Hair: 0.366\n",
            "Wavy_Hair: 0.165\n",
            "Wearing_Lipstick: 0.995\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Clinton/Chelsea_Clinton/Chelsea_Clinton_0001_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Clinton/Chelsea_Clinton/Chelsea_Clinton_0001.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Clinton/Bill_Clinton/Bill_Clinton_0001.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.031\n",
            "Arched_Eyebrows: 0.002\n",
            "Attractive: 0.002\n",
            "Bags_Under_Eyes: 0.852\n",
            "Bald: 0.004\n",
            "Bangs: 0.001\n",
            "Big_Lips: 0.022\n",
            "Big_Nose: 0.467\n",
            "Black_Hair: 0.007\n",
            "Blond_Hair: 0.020\n",
            "Blurry: 0.281\n",
            "Brown_Hair: 0.001\n",
            "Bushy_Eyebrows: 0.000\n",
            "Chubby: 0.522\n",
            "Double_Chin: 0.328\n",
            "Eyeglasses: 0.003\n",
            "Goatee: 0.002\n",
            "Gray_Hair: 0.746\n",
            "Heavy_Makeup: 0.000\n",
            "High_Cheekbones: 0.077\n",
            "Male: 0.999\n",
            "Mouth_Slightly_Open: 0.954\n",
            "Mustache: 0.006\n",
            "Narrow_Eyes: 0.011\n",
            "No_Beard: 0.941\n",
            "Oval_Face: 0.084\n",
            "Pale_Skin: 0.018\n",
            "Pointy_Nose: 0.018\n",
            "Receding_Hairline: 0.038\n",
            "Rosy_Cheeks: 0.001\n",
            "Sideburns: 0.007\n",
            "Smiling: 0.009\n",
            "Straight_Hair: 0.267\n",
            "Wavy_Hair: 0.022\n",
            "Wearing_Earrings: 0.001\n",
            "Wearing_Hat: 0.003\n",
            "Wearing_Lipstick: 0.000\n",
            "Wearing_Necklace: 0.001\n",
            "Wearing_Necktie: 0.073\n",
            "Young: 0.024\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.999\n",
            "Young: 1.000\n",
            "Smiling: 0.009\n",
            "Eyeglasses: 0.003\n",
            "Heavy_Makeup: 0.000\n",
            "Wearing_Hat: 0.003\n",
            "Oval_Face: 0.084\n",
            "Pointy_Nose: 0.018\n",
            "Mouth_Slightly_Open: 0.954\n",
            "Wearing_Earrings: 0.001\n",
            "Pale_Skin: 0.018\n",
            "5_o_Clock_Shadow: 0.031\n",
            "Chubby: 0.522\n",
            "Double_Chin: 0.328\n",
            "Goatee: 0.002\n",
            "Gray_Hair: 0.746\n",
            "Mustache: 0.006\n",
            "No_Beard: 0.941\n",
            "Receding_Hairline: 0.038\n",
            "Rosy_Cheeks: 0.001\n",
            "Straight_Hair: 0.267\n",
            "Wavy_Hair: 0.022\n",
            "Wearing_Lipstick: 0.000\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Clinton/Bill_Clinton/Bill_Clinton_0001_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Clinton/Bill_Clinton/Bill_Clinton_0001.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Reagan/Maureen_Reagan/Maureen_Reagan_0008.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.001\n",
            "Arched_Eyebrows: 0.475\n",
            "Attractive: 0.069\n",
            "Bags_Under_Eyes: 0.333\n",
            "Bald: 0.000\n",
            "Bangs: 0.957\n",
            "Big_Lips: 0.479\n",
            "Big_Nose: 0.132\n",
            "Black_Hair: 0.002\n",
            "Blond_Hair: 0.902\n",
            "Blurry: 0.221\n",
            "Brown_Hair: 0.003\n",
            "Bushy_Eyebrows: 0.005\n",
            "Chubby: 0.023\n",
            "Double_Chin: 0.084\n",
            "Eyeglasses: 0.000\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.072\n",
            "Heavy_Makeup: 0.454\n",
            "High_Cheekbones: 0.973\n",
            "Male: 0.005\n",
            "Mouth_Slightly_Open: 1.000\n",
            "Mustache: 0.000\n",
            "Narrow_Eyes: 0.802\n",
            "No_Beard: 0.997\n",
            "Oval_Face: 0.140\n",
            "Pale_Skin: 0.070\n",
            "Pointy_Nose: 0.120\n",
            "Receding_Hairline: 0.001\n",
            "Rosy_Cheeks: 0.007\n",
            "Sideburns: 0.001\n",
            "Smiling: 0.999\n",
            "Straight_Hair: 0.049\n",
            "Wavy_Hair: 0.327\n",
            "Wearing_Earrings: 0.122\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.837\n",
            "Wearing_Necklace: 0.082\n",
            "Wearing_Necktie: 0.001\n",
            "Young: 0.050\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.005\n",
            "Young: 1.000\n",
            "Smiling: 0.999\n",
            "Eyeglasses: 0.000\n",
            "Heavy_Makeup: 0.454\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.140\n",
            "Pointy_Nose: 0.120\n",
            "Mouth_Slightly_Open: 1.000\n",
            "Wearing_Earrings: 0.122\n",
            "Pale_Skin: 0.070\n",
            "5_o_Clock_Shadow: 0.001\n",
            "Chubby: 0.023\n",
            "Double_Chin: 0.084\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.072\n",
            "Mustache: 0.000\n",
            "No_Beard: 0.997\n",
            "Receding_Hairline: 0.001\n",
            "Rosy_Cheeks: 0.007\n",
            "Straight_Hair: 0.049\n",
            "Wavy_Hair: 0.327\n",
            "Wearing_Lipstick: 0.837\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Reagan/Maureen_Reagan/Maureen_Reagan_0008_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Reagan/Maureen_Reagan/Maureen_Reagan_0008.jpg\n",
            "Successfully loaded image from /content/drive/MyDrive/Child Generator/Families/Reagan/Patti_Davis/Patti_Davis_0001.jpg\n",
            "\n",
            "Raw Attribute Map:\n",
            "5_o_Clock_Shadow: 0.000\n",
            "Arched_Eyebrows: 0.018\n",
            "Attractive: 0.391\n",
            "Bags_Under_Eyes: 0.030\n",
            "Bald: 0.000\n",
            "Bangs: 0.001\n",
            "Big_Lips: 0.222\n",
            "Big_Nose: 0.011\n",
            "Black_Hair: 0.018\n",
            "Blond_Hair: 0.002\n",
            "Blurry: 0.147\n",
            "Brown_Hair: 0.513\n",
            "Bushy_Eyebrows: 0.002\n",
            "Chubby: 0.002\n",
            "Double_Chin: 0.002\n",
            "Eyeglasses: 0.000\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.000\n",
            "Heavy_Makeup: 0.216\n",
            "High_Cheekbones: 0.943\n",
            "Male: 0.002\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Mustache: 0.000\n",
            "Narrow_Eyes: 0.055\n",
            "No_Beard: 0.999\n",
            "Oval_Face: 0.070\n",
            "Pale_Skin: 0.013\n",
            "Pointy_Nose: 0.380\n",
            "Receding_Hairline: 0.004\n",
            "Rosy_Cheeks: 0.009\n",
            "Sideburns: 0.000\n",
            "Smiling: 0.997\n",
            "Straight_Hair: 0.319\n",
            "Wavy_Hair: 0.238\n",
            "Wearing_Earrings: 0.007\n",
            "Wearing_Hat: 0.001\n",
            "Wearing_Lipstick: 0.809\n",
            "Wearing_Necklace: 0.019\n",
            "Wearing_Necktie: 0.001\n",
            "Young: 0.286\n",
            "\n",
            "Mapped Attribute Values:\n",
            "Male: 0.002\n",
            "Young: 1.000\n",
            "Smiling: 0.997\n",
            "Eyeglasses: 0.000\n",
            "Heavy_Makeup: 0.216\n",
            "Wearing_Hat: 0.001\n",
            "Oval_Face: 0.070\n",
            "Pointy_Nose: 0.380\n",
            "Mouth_Slightly_Open: 0.999\n",
            "Wearing_Earrings: 0.007\n",
            "Pale_Skin: 0.013\n",
            "5_o_Clock_Shadow: 0.000\n",
            "Chubby: 0.002\n",
            "Double_Chin: 0.002\n",
            "Goatee: 0.000\n",
            "Gray_Hair: 0.000\n",
            "Mustache: 0.000\n",
            "No_Beard: 0.999\n",
            "Receding_Hairline: 0.004\n",
            "Rosy_Cheeks: 0.009\n",
            "Straight_Hair: 0.319\n",
            "Wavy_Hair: 0.238\n",
            "Wearing_Lipstick: 0.809\n",
            "\n",
            "Saved cropped face to /content/drive/MyDrive/Child Generator/Families/Reagan/Patti_Davis/Patti_Davis_0001_face.jpg\n",
            "Processed /content/drive/MyDrive/Child Generator/Families/Reagan/Patti_Davis/Patti_Davis_0001.jpg\n",
            "\n",
            "Processed 10 parent images and 10 child images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"# Convert images to vectors\"\"\"\n",
        "\n",
        "# Remove problematic imports and setup\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "# Define device (already defined above)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# We'll use the already loaded StyleGAN2-ADA generator (G) instead of loading a new one\n",
        "# The generator G is already properly initialized above\n",
        "\n",
        "# Use the same base path as defined in append_to_base_path function\n",
        "base_path = \"/content/drive/MyDrive/Child Generator/\"  # This matches the path used elsewhere in the code\n",
        "modified_children = [\n",
        "    [os.path.join(base_path, path[0]) for path in children if path]  # Add [0] to get first element and check if path exists\n",
        "]\n",
        "\n",
        "# Flatten the list and ensure each item is a string\n",
        "modified_children_flat = [item for sublist in modified_children for item in sublist if isinstance(item, str)]\n",
        "image_paths = parent_image_paths + modified_children_flat  # Combine parent and modified children paths\n",
        "print(image_paths)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((1024, 1024)),  # Resize to 1024x1024\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Generate X_data (latent vectors)\n",
        "X_data = []\n",
        "for image_path in tqdm_notebook(image_paths):\n",
        "    latent_vector = get_latent_vector(image_path, psp_encoder, device)  # Use psp_encoder instead of G\n",
        "    X_data.append(latent_vector.cpu().numpy())\n",
        "\n",
        "X_data = np.concatenate(X_data, axis=0)  # Concatenate into a single array\n"
      ],
      "metadata": {
        "id": "_kCaS03n8Nbw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "997a22de69fa45bd8b4b20c10e7e05c2",
            "a7d5c4bc678b4a0192a0a308341fd560",
            "34cc0be91a2a48e09d8b2f5c0e0a81aa",
            "09478b32cb1b41e5bec009e67603fbc5",
            "fb7cb3bacb7f4ab59af5277a9a71af54",
            "205cfef92fbd4198b7ac0e510f01377c",
            "d4ac98f4b22f4e56a1acbf0417dee1c7",
            "f21312d5f6f44ece90dcc15585a9ef68",
            "c4a7f3b6a11446298c577f0374c57bd7",
            "ae2ff5d360854a2aa550934daf16aca4",
            "af83f1847fa24a84b25a7eb2349412fb"
          ]
        },
        "outputId": "97af6797-ea9c-424f-d895-c87e83d13cd4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['generated_parents/combined_Barack_Obama_20403_Michelle_Obama_00007.jpg', 'generated_parents/combined_Barack_Obama_20091_Michelle_Obama_00015.jpg', 'generated_parents/combined_Barack_Obama_Sr_00002_Ann_Dunham_00013.jpg', 'generated_parents/combined_Lolo_Soetoro_00001_Ann_Dunham_00005.jpg', 'generated_parents/combined_Stanley_Dunham_00002_Madelyn_Payne_00002.jpg', 'generated_parents/combined_Jeb_Bush_0030_Columba_Bush_0001.jpg', 'generated_parents/combined_Bill_Clinton_0029_Hillary_Clinton_0005.jpg', 'generated_parents/combined_William_Jefferson_Blythe_0001_Virginia_Clinton_Kelley_0004.jpg', 'generated_parents/combined_Ronald_Reagan_0023_Jane_Wyman_0020.jpg', 'generated_parents/combined_Ronald_Reagan_0012_Nancy_Reagan_0014.jpg', '/content/drive/MyDrive/Child Generator/Families/Obama/Malia_Ann_Obama/Malia_Ann_Obama_00013.jpg', '/content/drive/MyDrive/Child Generator/Families/Obama/Natasha_Obama/Natasha_Obama_00019.jpg', '/content/drive/MyDrive/Child Generator/Families/Obama/Barack_Obama/Barack_Obama_20304.jpg', '/content/drive/MyDrive/Child Generator/Families/Obama/Maya_Soetoro/Maya_Soetoro_00021.jpg', '/content/drive/MyDrive/Child Generator/Families/Obama/Ann_Dunham/Ann_Dunham_00006.jpg', '/content/drive/MyDrive/Child Generator/Families/Bush/George_Prescott_Bush/George_Prescott_Bush_0008.jpg', '/content/drive/MyDrive/Child Generator/Families/Clinton/Chelsea_Clinton/Chelsea_Clinton_0001.jpg', '/content/drive/MyDrive/Child Generator/Families/Clinton/Bill_Clinton/Bill_Clinton_0001.jpg', '/content/drive/MyDrive/Child Generator/Families/Reagan/Maureen_Reagan/Maureen_Reagan_0008.jpg', '/content/drive/MyDrive/Child Generator/Families/Reagan/Patti_Davis/Patti_Davis_0001.jpg']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "997a22de69fa45bd8b4b20c10e7e05c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"# Train classifier on each feature\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import r2_score, accuracy_score\n",
        "\n",
        "# List of features\n",
        "features = [\n",
        "    '5_o_Clock_Shadow', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee',\n",
        "    'Gray_Hair', 'Heavy_Makeup', 'Male', 'Mouth_Slightly_Open',\n",
        "    'Mustache', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose',\n",
        "    'Receding_Hairline', 'Rosy_Cheeks', 'Smiling', 'Straight_Hair',\n",
        "    'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
        "    'Young'\n",
        "]\n",
        "\n",
        "# Features that should be **classified** instead of regressed\n",
        "binary_features = {'Eyeglasses', 'Heavy_Makeup', 'Smiling', 'Wearing_Hat', 'Male'}\n",
        "\n",
        "# Extract numerical labels\n",
        "def extract_labels(data, features):\n",
        "    \"\"\"Convert a list of (attribute_dict, image_path) tuples into a NumPy array of labels.\"\"\"\n",
        "    return np.array([[sample[0].get(feature, 0) for feature in features] for sample in data])\n",
        "\n",
        "# Convert parent and child labels to NumPy arrays\n",
        "parent_labels_np = extract_labels(parent_labels, features)\n",
        "child_labels_np = extract_labels(child_labels, features)\n",
        "\n",
        "# Combine data\n",
        "X_data = np.vstack((parent_labels_np, child_labels_np))  # Shape: (num_samples, num_features)\n",
        "\n",
        "# Dictionary to store trained models\n",
        "feature_models = {}\n",
        "\n",
        "# Train regressors/classifiers for each feature\n",
        "for i, feature in enumerate(features):\n",
        "    if i < X_data.shape[1]:  # Ensure feature exists\n",
        "        y = X_data[:, i]\n",
        "\n",
        "        # **Binarize binary features**\n",
        "        if feature in binary_features:\n",
        "            y = (y >= 0.5).astype(int)  # Convert probability to 0 or 1\n",
        "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        else:\n",
        "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        if feature in binary_features:\n",
        "            train_score = accuracy_score(y_train, model.predict(X_train))\n",
        "            test_score = accuracy_score(y_test, model.predict(X_test))\n",
        "            print(f\"✔ {feature} classifier trained\")\n",
        "        else:\n",
        "            train_score = model.score(X_train, y_train)\n",
        "            test_score = model.score(X_test, y_test)\n",
        "            print(f\"✔ {feature} regressor trained\")\n",
        "\n",
        "        print(f\"   ↳ Training Score: {train_score:.3f}\")\n",
        "        print(f\"   ↳ Testing Score: {test_score:.3f}\\n\")\n",
        "\n",
        "        # Store model\n",
        "        feature_models[feature] = model\n",
        "    else:\n",
        "        print(f\"⚠ Warning: No labels found for {feature}, skipping.\")\n",
        "\n",
        "print(\"\\n🎯 All models trained successfully!\")"
      ],
      "metadata": {
        "id": "Ol4h9dVe8e32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957f6547-a2e6-4725-931b-6fd7b2975d03"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ 5_o_Clock_Shadow regressor trained\n",
            "   ↳ Training Score: 0.947\n",
            "   ↳ Testing Score: -0.226\n",
            "\n",
            "✔ Chubby regressor trained\n",
            "   ↳ Training Score: 0.970\n",
            "   ↳ Testing Score: 0.816\n",
            "\n",
            "✔ Double_Chin regressor trained\n",
            "   ↳ Training Score: 0.970\n",
            "   ↳ Testing Score: 0.974\n",
            "\n",
            "✔ Eyeglasses classifier trained\n",
            "   ↳ Training Score: 1.000\n",
            "   ↳ Testing Score: 1.000\n",
            "\n",
            "✔ Goatee regressor trained\n",
            "   ↳ Training Score: 0.891\n",
            "   ↳ Testing Score: -0.261\n",
            "\n",
            "✔ Gray_Hair regressor trained\n",
            "   ↳ Training Score: 0.842\n",
            "   ↳ Testing Score: -0.044\n",
            "\n",
            "✔ Heavy_Makeup classifier trained\n",
            "   ↳ Training Score: 1.000\n",
            "   ↳ Testing Score: 1.000\n",
            "\n",
            "✔ Male classifier trained\n",
            "   ↳ Training Score: 1.000\n",
            "   ↳ Testing Score: 1.000\n",
            "\n",
            "✔ Mouth_Slightly_Open regressor trained\n",
            "   ↳ Training Score: 0.985\n",
            "   ↳ Testing Score: 0.716\n",
            "\n",
            "✔ Mustache regressor trained\n",
            "   ↳ Training Score: 0.925\n",
            "   ↳ Testing Score: 0.723\n",
            "\n",
            "✔ No_Beard regressor trained\n",
            "   ↳ Training Score: 0.925\n",
            "   ↳ Testing Score: -0.264\n",
            "\n",
            "✔ Oval_Face regressor trained\n",
            "   ↳ Training Score: 0.956\n",
            "   ↳ Testing Score: 0.737\n",
            "\n",
            "✔ Pale_Skin regressor trained\n",
            "   ↳ Training Score: 0.875\n",
            "   ↳ Testing Score: -9.239\n",
            "\n",
            "✔ Pointy_Nose regressor trained\n",
            "   ↳ Training Score: 0.874\n",
            "   ↳ Testing Score: 0.888\n",
            "\n",
            "✔ Receding_Hairline regressor trained\n",
            "   ↳ Training Score: 0.965\n",
            "   ↳ Testing Score: 0.699\n",
            "\n",
            "✔ Rosy_Cheeks regressor trained\n",
            "   ↳ Training Score: 0.929\n",
            "   ↳ Testing Score: -92.000\n",
            "\n",
            "✔ Smiling classifier trained\n",
            "   ↳ Training Score: 1.000\n",
            "   ↳ Testing Score: 0.750\n",
            "\n",
            "✔ Straight_Hair regressor trained\n",
            "   ↳ Training Score: 0.972\n",
            "   ↳ Testing Score: 0.780\n",
            "\n",
            "✔ Wavy_Hair regressor trained\n",
            "   ↳ Training Score: 0.922\n",
            "   ↳ Testing Score: -713.243\n",
            "\n",
            "✔ Wearing_Earrings regressor trained\n",
            "   ↳ Training Score: 0.887\n",
            "   ↳ Testing Score: -0.530\n",
            "\n",
            "✔ Wearing_Hat classifier trained\n",
            "   ↳ Training Score: 1.000\n",
            "   ↳ Testing Score: 1.000\n",
            "\n",
            "✔ Wearing_Lipstick regressor trained\n",
            "   ↳ Training Score: 0.989\n",
            "   ↳ Testing Score: -6.877\n",
            "\n",
            "✔ Young regressor trained\n",
            "   ↳ Training Score: 1.000\n",
            "   ↳ Testing Score: 1.000\n",
            "\n",
            "\n",
            "🎯 All models trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Edit parent image (into generated child)\n",
        "# Compare to real child\n",
        "# Adjust model\n",
        "\"\"\"\n",
        "\n",
        "# Replace your existing feature direction functions with these\n",
        "\n",
        "def get_feature_direction(classifier):\n",
        "    \"\"\"Extract feature direction from classifier\"\"\"\n",
        "    if hasattr(classifier, 'coef_'):\n",
        "        # For linear models like LogisticRegression\n",
        "        return classifier.coef_[0]\n",
        "    elif hasattr(classifier, 'feature_importances_'):\n",
        "        # For tree-based models like RandomForest\n",
        "        # Project feature importances to latent dimension\n",
        "        importances = classifier.feature_importances_\n",
        "        # Expand to match latent dimension by repeating\n",
        "        expanded = np.repeat(importances, 512 // len(importances) + 1)[:512]\n",
        "        return expanded\n",
        "    else:\n",
        "        raise ValueError(\"Classifier doesn't have a recognized direction attribute\")\n",
        "\n",
        "def modify_feature(w_vector, direction, strength=1.0):\n",
        "    \"\"\"Modify a W+ latent vector along a feature direction\n",
        "\n",
        "    Args:\n",
        "        w_vector: The latent vector to modify (1 x num_ws x 512)\n",
        "        direction: The direction to modify along (512)\n",
        "        strength: The strength of the modification\n",
        "\n",
        "    Returns:\n",
        "        Modified latent vector\n",
        "    \"\"\"\n",
        "    # Make a deep copy to avoid modifying the original\n",
        "    modified = w_vector.clone()\n",
        "\n",
        "    # Ensure direction is the right size and type\n",
        "    direction = torch.tensor(direction, device=modified.device, dtype=modified.dtype)\n",
        "\n",
        "    # Create a new tensor for the modifications\n",
        "    modified_tensor = modified.clone()\n",
        "\n",
        "    # Apply the modification to each layer\n",
        "    for i in range(modified.size(1)):\n",
        "        layer = modified[:, i].clone()\n",
        "        direction_reshaped = direction.view_as(layer)\n",
        "        modified_tensor[:, i] = layer + direction_reshaped * strength\n",
        "\n",
        "    return modified_tensor\n"
      ],
      "metadata": {
        "id": "Ws5WIpfh8iTY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_feature_strengths(parent_pairs, child_images, feature_models, G, device, num_epochs=100):\n",
        "    \"\"\"Train feature strength parameters to match child attributes.\n",
        "\n",
        "    Args:\n",
        "        parent_pairs: List of (father_path, mother_path) tuples\n",
        "        child_images: List of corresponding child image paths\n",
        "        feature_models: Dictionary of trained feature classifiers\n",
        "        G: StyleGAN2 generator\n",
        "        device: Torch device\n",
        "        num_epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of optimized feature strengths\n",
        "    \"\"\"\n",
        "    # Initialize feature strengths as learnable parameters\n",
        "    feature_strengths = {\n",
        "        feature: torch.tensor(0.5, device=device, requires_grad=True)\n",
        "        for feature in feature_models.keys()\n",
        "    }\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.Adam(feature_strengths.values(), lr=0.01)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for (father_path, mother_path), child_path in zip(parent_pairs, child_images):\n",
        "            # Get parent latents\n",
        "            father_latent = get_latent_vector(father_path, psp_encoder, device)\n",
        "            mother_latent = get_latent_vector(mother_path, psp_encoder, device)\n",
        "\n",
        "            # Initial child latent (average of parents)\n",
        "            child_latent = (father_latent + mother_latent) / 2\n",
        "\n",
        "            # Apply feature modifications\n",
        "            for feature, model in feature_models.items():\n",
        "                direction = get_feature_direction(model)\n",
        "                strength = feature_strengths[feature]\n",
        "                child_latent = modify_feature(child_latent, direction, strength)\n",
        "\n",
        "            # Generate edited image\n",
        "            with torch.no_grad():\n",
        "                edited_image = G.synthesis(child_latent)\n",
        "\n",
        "            # Get attributes for edited and real child images\n",
        "            edited_attrs, _ = analyze_face(tensor_to_pil(edited_image))\n",
        "            real_attrs, _ = analyze_face(child_path)\n",
        "\n",
        "            if edited_attrs is None or real_attrs is None:\n",
        "                continue\n",
        "\n",
        "            # Calculate attribute matching loss\n",
        "            loss = 0\n",
        "            for feature in feature_models.keys():\n",
        "                if feature in edited_attrs and feature in real_attrs:\n",
        "                    loss += (edited_attrs[feature] - real_attrs[feature]) ** 2\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        avg_loss = total_loss / len(parent_pairs)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Print current feature strengths\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(\"\\nCurrent feature strengths:\")\n",
        "            for feature, strength in feature_strengths.items():\n",
        "                print(f\"{feature}: {strength.item():.3f}\")\n",
        "\n",
        "    # Return optimized strengths as regular dictionary\n",
        "    return {k: v.item() for k, v in feature_strengths.items()}"
      ],
      "metadata": {
        "id": "ItHqT4ysX5W0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"# Lil Demo\"\"\"\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_child(father_img, mother_img, feature_models, encoder, generator, feature_strengths=None):\n",
        "    \"\"\"Generate a child image from parent images using trained feature models.\n",
        "\n",
        "    Args:\n",
        "        father_img: PIL Image or path to father's image\n",
        "        mother_img: PIL Image or path to mother's image\n",
        "        feature_models: Dictionary of trained feature models\n",
        "        encoder: pSp encoder for extracting latent vectors\n",
        "        generator: StyleGAN2 generator for generating images\n",
        "        feature_strengths: Optional dictionary of feature strengths\n",
        "\n",
        "    Returns:\n",
        "        Generated child image as PIL Image\n",
        "    \"\"\"\n",
        "    # Get parent latent vectors using the encoder\n",
        "    father_latent = get_latent_vector(father_img, encoder, device)\n",
        "    mother_latent = get_latent_vector(mother_img, encoder, device)\n",
        "\n",
        "    # Initial child latent (average of parents)\n",
        "    child_latent = (father_latent + mother_latent) / 2\n",
        "\n",
        "    # If no feature strengths provided, use default\n",
        "    if feature_strengths is None:\n",
        "        feature_strengths = {feature: 0.5 for feature in feature_models.keys()}\n",
        "\n",
        "    # Apply feature modifications\n",
        "    for feature, model in feature_models.items():\n",
        "        direction = get_feature_direction(model)\n",
        "        strength = feature_strengths[feature]\n",
        "        child_latent = modify_feature(child_latent, direction, strength)\n",
        "\n",
        "    # Generate child image using the generator\n",
        "    with torch.no_grad():\n",
        "        child_image = generator.synthesis(child_latent)\n",
        "\n",
        "    # Convert to PIL image\n",
        "    child_image = child_image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "    child_image = ((child_image + 1) * 127.5).astype(np.uint8)\n",
        "    child_image = Image.fromarray(child_image)\n",
        "\n",
        "    return child_image\n",
        "\n",
        "def demo_child_generation():\n",
        "    \"\"\"Interactive demo for child generation from uploaded parent photos.\"\"\"\n",
        "    print(\"Please upload two photos: father and mother\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get uploaded images\n",
        "    image_paths = list(uploaded.keys())\n",
        "    if len(image_paths) != 2:\n",
        "        print(\"Error: Please upload exactly 2 images\")\n",
        "        return\n",
        "\n",
        "    father_path = image_paths[0]\n",
        "    mother_path = image_paths[1]\n",
        "\n",
        "    # Load images\n",
        "    father_img = Image.open(io.BytesIO(uploaded[father_path])).convert('RGB')\n",
        "    mother_img = Image.open(io.BytesIO(uploaded[mother_path])).convert('RGB')\n",
        "\n",
        "    # Generate child\n",
        "    child_img = generate_child(father_img, mother_img, feature_models, psp_encoder, G)\n",
        "    # Display results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(father_img)\n",
        "    plt.title(\"Father\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mother_img)\n",
        "    plt.title(\"Mother\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(child_img)\n",
        "    plt.title(\"Generated Child\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cRgs0FKO8llC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_pil(tensor):\n",
        "    \"\"\"Convert a tensor image to PIL Image.\"\"\"\n",
        "    image = tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        "    image = ((image + 1) * 127.5).astype(np.uint8)\n",
        "    return Image.fromarray(image)"
      ],
      "metadata": {
        "id": "-H5KOWPTYA0W"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "        '5_o_Clock_Shadow', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee',\n",
        "        'Gray_Hair', 'Heavy_Makeup', 'Male', 'Mouth_Slightly_Open',\n",
        "        'Mustache', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose',\n",
        "        'Receding_Hairline', 'Rosy_Cheeks', 'Smiling', 'Straight_Hair',\n",
        "        'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
        "        'Young'\n",
        "    ]\n",
        "\n",
        "# Train models if not already trained\n",
        "if 'feature_models' not in globals():\n",
        "    print(\"Training feature models...\")\n",
        "    feature_models = train_feature_models(parent_labels, child_labels, features)\n",
        "\n",
        "# Run demo\n",
        "demo_child_generation()"
      ],
      "metadata": {
        "id": "NZtau8w38p-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "0412208b-cfb6-4788-871a-6fe3f052fb79"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload two photos: father and mother\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d4e0b326-7184-4ee7-b6f2-945b5ef96f6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d4e0b326-7184-4ee7-b6f2-945b5ef96f6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fathertest.jpg to fathertest.jpg\n",
            "Saving mothertest.jpg to mothertest.jpg\n",
            "W+ shape: torch.Size([1, 18, 512])\n",
            "W+ shape: torch.Size([1, 18, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A-WZw8Zpadas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tnQ-I8yYadvi"
      }
    }
  ]
}